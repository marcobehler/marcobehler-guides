= Kubernetes for Lazy Developers
Marco Behler
2024-11-11
:page-layout: layout-guides
:page-image: "TODO"
:page-description: TODO
:page-published: false
:page-tags: ["kuberntes"]
:page-commento_id: /guides/kubernetes-for-lazy-developer

(*Editor’s note*: At ~n words, you probably don't want to try reading this on a mobile device. Bookmark it and come back later.)

== Introduction

If you're a developer who has either never used Kubernetes before or wants to brush up on Kubernetes knowledge so that your DevOps colleagues will be [line-through]#scared# positively surprised, this guide is for you.

*Quick Question*: Why is Kubernetes abbreviated to K8s?

*Answer*: It will magically reveal itself at the end of this guide, but only after you having fully read it. So, let's not waste any more time.

=== How did we get here?

Many developers I've met throughout my career, didn't necessarily care about the "now that I've written the code, it also needs to be run somewhere"-part of your application's life.

Let's start slowly. What does running an application actually mean?

It depends a bit on your programming language ecosystem, but, for example, in terms of Java and modern web applications, you literally compile all your source in one, single, executable .jar file, that you can run with a simple command.

[source,java]
----
java -jar myNewAIStartup.jar
----

For production deployments, you typically also want some sort of properties, to set e.g. database credentials or other secrets. The easiest way in above's Java Spring Boot case would be to have an `_application.properties_` file, but you can also go crazy with external secret management services.

In any case, the command above is literally all you need to run to deploy your application - doesn't matter if you're deploying on bare metal, a VM, a Docker container, with or without Kubernetes or maybe even your Java-powered toaster.

=== Running just one command can't be it. Where is the catch?

Much has been written online about the issues that can arise when deploying your application:

* What if there are library/OS/infrastructure/something version incompatibilities between my DEV environment and my PRD environment?
* What if certain required OS packages are missing?
* What if we don't want deploy our application to a cloud service anymore, but a submarine?

Your mileage with these problems will vary _a lot_ depending on what programming language ecosystem you're using - a topic that is often ignored in online discussions.

With pure Java web-applications, the issues mentioned above (excluding the submarine or more sensible infrastructure parts like your database), are, for a large part, non-issues. You https://www.marcobehler.com/guides/a-guide-to-java-versions-and-features[install a JDK] on a server/vm/container/submarine, run `_java -jar_` and live happily ever after.

With PHP, Python, ahem, Node the picture looks different and you could/can easily run into version incompatibilities across your dev, staging or prod environments. Hence, people yearned for a solution to rid them of these pains: Containers.

=== Hail to the Docker!

Chances are high that you already know what Docker is and how to work with it. (If not, and you want to see a Docker specific guide, please post a comment down below! More demand -> Faster publication). The short summary is:

* You build your source code into a deployable, e.g. the `_jar_` file.
* You additionally build a new Docker image, with your `_jar_` file baked in.
* Said Docker image also contains _all_ additional software and configuration options needed to be able to run successfully.

-> Instead of deploying your .jar file, you'll now deploy your Docker image and run a Docker container.

The beauty of this is: As long as you have Docker installed on the target machine (and you have Kernel compatibility between your host OS and the Docker container) you can run any Docker image you want.

[source,console]
----
docker run --name my-new-ai-startup -p 80:8080 -d mynewaistartup

// yay, your application is deployed

// This will start a Docker container based on the `_mynewaistartup_` Docker image.
// That image will, among other things, contain your e.g. -jar file,
// a web application which runs on port 8080, as well as instructions on how to run it.
// Hint: These instructions are `_java -jar mynewaistartup.jar_`.
----

=== What does 'deploying' Docker images mean?

You or your CI/CD server managed to bake your application into a Docker image. But how does that Docker image eventually end up on your target deployment server?

You could theoretically save your Docker image as a .tar file, copy it onto your final server and load it there. But more realistically, you'd push your Docker image to a Docker registry, be that https://hub.docker.com/_/registry[dockerhub], https://aws.amazon.com/ecr/[Amazon ECR] or any of the other gazillion self-hosted container registries, like https://docs.gitlab.com/ee/user/packages/container_registry/[Gitlab's], for example.

Once your image made it to said registry, you can https://docs.docker.com/engine/reference/commandline/login/[login] to that registry on your target server.

[source,console]
----
docker login mysupersecret.registry.com
----

And once you are logged in to your registry, your `_docker run_` will be able to find your custom images.

[source,console]
----
docker run --name my-new-ai-startup -p 80:8080 -d mynewaistartup

//....
SUCCESS!
----

=== Docker Compose: Running more than one container simultaneously

What if your application consists of more than just a single Docker container, say, because you need to run https://www.marcobehler.com/guides/java-microservices-a-practical-guide[98354 microservices]?

https://docs.docker.com/compose/[Docker Compose] to the rescue. You'll define all your services and the dependencies between them (run this one or the other one first) in a good, old, YAML file, a `_compose.yaml_`. Here's an example of such a file, defining two services, a web service and a redis service.

[source,yaml]
----
services:
  web:
    build: .
    ports:
      - "8000:5000"
    volumes:
      - .:/code
      - logvolume01:/var/log
    depends_on:
      - redis
  redis:
    image: redis
volumes:
  logvolume01: {}
----

Then you just run `_docker compose up_` and your whole _environment_ (consisting of all your separate services running in separate docker containers) will be started.

While `_Docker Compose_` might be mainly known for quickly spinning up development or testing environments, it actually works really well for single host deployments as well.
If your application...

* doesn't have any specific high-availability requirements
* you don't mind some manual work (ssh login, docker compose up/down) or using a complementary tool like https://www.ansible.com/[Ansible]
* or you simply don't want to spend enormous amounts of [line-through]#money# investments on a DevOps team

...using Docker Compose for production deployments will go a long way.

=== What do I need Kubernetes for, then?

Things get interesting if you want to start running hundreds, thousands (or a multiple of that) containers, if you don't care or don't want to know on what specific underlying hardware/box your containers will be running on, yet still want to be able to sensibly manage all of this. Kubernetes, to the rescue!

(Note: Quite a while ago I read a Kubernetes book, where in the intro they specified a lower-bound number where running Kubernetes starts makes sense and *I think* it started with hundreds to thousands, though I cannot find the exact book anymore. To be checked)

Let's do a quick Kubernetes Concept 101.

TODO GRAPH UML control pane workload..

=== Kubernetes 101: (Worker) Nodes

Your software (or _workload_ in Kubernetes terms) has to run somewhere, be it a virtual or physical machine. Kubernetes call this somewhere `_Nodes_`.

Furthermore, Kubernetes deploys and runs containers: Hello, Docker, my old friend!

Actually, this is not 100% right. In Kubernetes' terms, you deploy (_schedule_) `_Pods_`, with a pod consisting of one or more containers.

Alright, we got `_pods_` running on `_nodes_`, but who controls those nodes and how and where do you decide what to run on these `_nodes_`?

=== Kubernetes 101: Control Plane

Meet the `_Control Plane_` For simplicity's sake, let's just think of it as _one_ component that controls your nodes (as opposed to the https://kubernetes.io/docs/concepts/overview/components/[approximately 9472 components] it consists of). The control pane, among many things,...

* Lets you [line-through]#run# _schedule_ your application, i.e. lets you put a pod on a node.
* Checks if all your pods are in the desired state, e.g. are they responsive or does one of them need to be restarted?
* Fulfills every engineer's wet dream: We need to finally scale 10xfold, let's quickly spin up n-more pods!

=== Kubernetes 101: Clusters & Clouds

Take multiple nodes and your control pane, and you have a `_cluster_`.

Take multiple clusters and you can separate your dev, test & production environments or maybe teams, projects or different application types - that's up to you.

Take it even further, and [line-through]#try going# go multi-cloud Kubernetes, running multiple clusters across different private and/or public cloud platforms (Congratulations! What you have achieved is not for the faint of heart)

=== Kubernetes 101: Addons

There are also a fair amount of https://kubernetes.io/docs/concepts/overview/components/#web-ui-dashboard[Kubernetes add-ons].

Most importantly for developers there is a https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/[Web UI/Dashboard] which you can use to essentially manage your cluster.

If you're not self-hosting your Kubernetes setup, you'd simply use whatever UI your cloud vendors, like https://cloud.google.com/gcp/[Google Cloud], https://aws.amazon.com/eks/[AWS] or the many others provide.

=== Please, let's stop with the Kubernetes 101

Those four 101-sections above will (hopefully) give you enough of a mental model to get started with Kubernetes and we'll leave it at that with the concepts.

Truth to be told, you'll be shocked if you enter "Kubernetes" on https://learning.oreilly.com . You'll get thousands of learning resource results, with many many many of the books being 500+ pages long. Fine reading for a rainy winter day! The good part: you, as a developer, don't have to care about most of that's written in these books, teaching you how to setup, maintain and manage your clusters, though being aware of the sheer complexity of all of this helps.

=== What do I need to do to see all of this in action?

* A Kubernetes installation (we'll talk about that a bit later in more detail)
* YAML, YAML, YAML!!!
* A tool to interact with your Kubernetes cluster: `_kubectl_`

=== Where do I get kubectl?

You can download `_kubectl_`, which is essentially _the_ CLI tool to do everything you ever wanted to do with your Kubernetes cluster https://kubernetes.io/docs/tasks/tools/[here]. That page lists various ways of installing kubectl for your specific operating system.

=== What do I need for kubectl to work?

You'll need a config file, a so-called `_kubeconfig file_`, which lets you access your Kubernetes cluster.

By default, that file is located at `_~/.kube/config_`. It is also important to note that this config file is also being read in by your favorite IDE, like https://www.jetbrains.com/idea/[IntelliJ IDEA], to properly set up its Kubernetes features.

=== Where do I get the kubeconfig file from?

*Option 1* If you are using a managed Kubernetes installation (https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html[EKS], https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl[GKE], https://gist.github.com/dcasati/c71243c1a010993d9f281e0f06dc839d[AKS]), check out the corresponding documentation pages. Yes, just click the links, I did all the work linking to the correct pages. Simply put, you'll use their CLI tools to generate/download the file for you.

*Option 2* If you installed e.g. https://minikube.sigs.k8s.io/docs/start/[Minikube] locally, it will automatically create a kubeconfig file for you.

*Option 3* If you happen to know your Kubernetes master node and can ssh into it, run a:

`_cat /etc/kubernetes/admin.conf_` or cat `_~/.kube/config_`

=== Anything else I need to know about the kubeconfig file?

A kubeconfig file is good, old YAML, and there are many things it can contain (clusters, users, contexts). For the inclined, https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/[check out the official documentation].

For now we can ignore users and contexts and live with the simplification that the kubeconfig file contains the cluster(s) you can connect to, e.g. `_development_` or `_test_`.

Here's an example kubeconfig file, taken https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/[from the official Kubernetes documentation].

[source,yaml]
----
apiVersion: v1
clusters:
- cluster:
    certificate-authority: fake-ca-file
    server: https://1.2.3.4
  name: development
- cluster:
    insecure-skip-tls-verify: true
    server: https://5.6.7.8
  name: test
contexts:
- context:
    cluster: development
    namespace: frontend
    user: developer
  name: dev-frontend
- context:
    cluster: development
    namespace: storage
    user: developer
  name: dev-storage
- context:
    cluster: test
    namespace: default
    user: experimenter
  name: exp-test
current-context: ""
kind: Config
preferences: {}
users:
- name: developer
  user:
    client-certificate: fake-cert-file
    client-key: fake-key-file
- name: experimenter
  user:
    # Documentation note (this comment is NOT part of the command output).
    # Storing passwords in Kubernetes client config is risky.
    # A better alternative would be to use a credential plugin
    # and store the credentials separately.
    # See https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins
    password: some-password
    username: exp
----

TODO line by line explanations:

=== Kubectl 101

What can you now do with Kubectl? Remember, at the beginning we said your goal is to have a pod (n+ containers), and schedule it (run them) on a node (server).

And the way is to feed yaml files (yay) with the desired state of your cluster into kubectl, and it will happily set your cluster into the desired state.


=== Pod Manifests

You could, for example, create a file called `_marcocodes-pod.yaml_` that looks like this...

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: marcocodes-web
spec:
  containers:
    - image: gcr.io/marco/marcocodes:1.4
      name: marcocodes-web
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
----

...and feed it into your Kubernetes cluster with the following kubectl command:

[source,console]
----
 kubectl apply -f marcocodes-pod.yaml
----

What would applying this yaml file do? Well, let's go through it step by step:

[source,yaml,indent=0,role=tooth]
----
kind: Pod
----

Kubernetes knows a variety of so-called `_objects_`, `_Pod_` being one of them, and you'll meet the other ones in a bit. Simply put, this .yaml file describes what pod we want to deploy.

[source,yaml,indent=0,role=tooth]
----
metadata:
  name: marcocodes-web
----

Every object and thus every .yaml file in Kubernetes is full of `_metadata_` tags. In this case, we give our pod the `_name_` with value `_marcocodes_web_`. What is this metadata for?
Simply put, Kubernetes needs to somehow, uniquely identify resources in a cluster: Do I already have a pod with name `_marcocodes_web_` running or do I have to start a new one? That is what the metadata is for.


[source,yaml,indent=0,role=tooth]
----
spec:
  containers:
    - image: gcr.io/marco/marcocodes:1.4
      name: marcocodes-web
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
----

You need to tell Kubernetes _what_ your pod should look like. Remember, it can be n+ containers, hence you can specify a list of containers in the YAML file, even though often you only specify exactly one.

You'll specify a specific Docker image, including its version and also expose port 8080 on that container via http. That's it.

*Long Story Short* When you run `_kubectl apply_`, your yaml file will be submitted to the https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/[Kubernetes API Server] and eventually our Kubernetes system will schedule a pod (with a marcocodes 1.4 container) to run on a healthy, viable node in our cluster.

=== Resources & Volumes

Specifying just the container image isn't all you can do. First off, you might want to take care of your container's resource consumption:

[source,yaml,indent=0,role=tooth]
----
# ....
spec:
  containers:
    - image: gcr.io/marco/marcocodes:1.4
      resources:
        requests:
          cpu: "500m"
          memory: "128Mi"
# ....
----

This makes sure your container gets _at least_ _500m_ (aka 0,5) of CPU, and 128 MB of memory. (You can also specify upper limits that are never to be broken).

In addition, when a Pod is deleted or a container simply restarts, the data in the container’s filesystem is deleted. To circumvent that, you might want to store your data on a `_persistent volume_`.

[source,yaml,indent=0,role=tooth]
----
# ....
spec:
  volumes:
    - name: "marcocodes-data"
      hostPath:
        path: "/var/lib/marcocodes"
  containers:
    - image: gcr.io/marco/marcocodes:1.4
      name: marcocodes
      volumeMounts:
        - mountPath: "/data"
          name: "marcocodes-data"
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
# ....
----

We're going to have one volume called `_marcocodes-data_`, which will be mounted to the `_/data_` directory on the container, and live under `_/var/lib/marcocodes_` on the host machine.

=== Where's the catch?

You just learned that there are pods, and they consist of one or more Docker images, as well as resource consumption rules and volume definitions.

With all of that YAML we then managed to schedule a single, static, one-off pod. Cheeky question: Where is the advantage to just running `_docker run -d --publish 8080:8080 gcr.io/marco/marcocodes:1.4_`?

Well, for now, there actually is none.

That's why we need to dig deeper into the concepts of `_ReplicaSets_` and `_Deployments_

=== ReplicaSets

Let's be humble. We don't need auto-scaling right off the bat, but it would be nice to have redundant instances of our application and some load-balancing, to be a bit more professional with our deployments, wouldn't it?

Kubernetes' `_ReplicaSets_` to the rescue!

Let's have a look at a `_marcocodes-replica.yaml_` file, that defines such a minimal ReplicaSet.

[source,yaml]
----
apiVersion: apps/v1
kind: ReplicaSet
# metadata:
# ...
spec:
  replicas: 2
  selector: "you will learn this later"
  # ...
  template:
    metadata: "you will learn this later"
      # ...
    spec:
      containers:
        - name: marcocodes-web
          image: "gcr.io/marco/marcocodes:3.85"
----

I left out a fair amount of lines (and complexity) out of this YAML file, but most interestingly for now are these two changes:

[source,yaml,indent=0,role=tooth]
----
kind: ReplicaSet
----

This .yaml now describes a `_ReplicaSet_`, not a `_Pod_` anymore.

[source,yaml,indent=0,role=tooth]
----
spec:
  replicas: 2
----

Here's the meat: We want to have 2 replicas == pods running at any given time. If we put in 10 here, Kubernetes would make sure to have 10 pods running at the same time.

When we now apply this .yaml file....

[source,console]
----
kubectl apply -f marcocodes-rs.yaml
----

Kubernetes will fetch a Pod listing from the Kubernetes API (and filter the results by metadata) and depending on the number of pods being returned, Kubernetes will spin up or down additional replicas. That's all there is to it.

=== ReplicaSets: Summary

`_ReplicaSets_` are _almost_ what you'd like to have, but they come with a problem: They are tied to a specific version of your container images (3.85 in our case above) and those are actually not really expected to change. And ReplicaSets also don't really help you with the _rolling out process_ (think, zero downtime) of your software.

Hence we need a new concept to help us manage releases of new versions, meet `_Deployments_`.

=== Deployments

Meet `_Deployments_`, which are used to manage `_ReplicaSets_` (wow!).

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata: "ignore for now"
  # ...
spec:
  progressDeadlineSeconds: 600
  replicas: 2
  revisionHistoryLimit: 10
  selector: "ignore for now"
    # ...
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
     "ignore for now"
    # ...
----

There are an additional 92387 YAML key-value pairs you'll need to learn for Deployments, and we're already quite long into this article. The gist of it is: Kubernetes allows you to have different software rollout strategies (`_rollingUpdate_` or `_recreate_`).

* _Recreate_ will kill all your pods with the old version and re-create them with a new version: your users will experience downtime
* _RollingUpdate_ will perform the update while still serving traffic through old pods, and is thus generally preferred.


=== TODO: Reconcilliation loops

TODO TODO: where does this fit in?

In a reconciliation loop, the scheduler says “here is the user’s desired state. Here is the current state. They are not the same, so I will take steps to reconcile them.” The user wants storage for the container. Currently there is no storage attached. So Kubernetes creates a unit of storage and attaches it to the container. The container needs a public network address. None exists. So a new address is attached to the container. Different subsystems in Kubernetes work to fulfill their individual part of the user’s overall declaration of desired state.


=== Rolling Updates: Too Good To Be True

As always, the devil is in the details. Rolling updates have been done many moons ago already, before Kubernetes existed, even if it was just batch scripts firing SSH commands.

The issue, bluntly put, is not so much about being able to shut down and spin up new instances of your application, but that for a short while (during the deployment) your app essentially needs to gracefully support two versions of the application - which is always interesting as soon as a database is involved or if there have been major refactorings in APIs between frontend/backend, for example.

So, beware of the marketing materials, selling you easy rolling updates - their real challenge has nothing to do with Kubernetes.

=== Side-Note: Self-Healing

On a similar note, that same is true for the term _self-healing_. What Kubernetes can do, is execute health-checks and then take an unresponsive pod, kill it, and schedule a new one. That is also functionality, which has in one form or another existed endlessly (TODO systemd comparison, across multiple nodes etc). What Kubernetes _cannot_ do is automatically take a botched database migration, which leads to application errors and then magically _self-heal_ the cluster.

It is just my personal impression that talk about _self-healing_ systems often insinuates the latter (maybe among management), whereas it is much more basic functionality.

=== Service Discovery, Load Balancing & Ingress

So far, we talked about dynamically spawning up pods, but never about how network traffic actually reaches your applications. Kubernetes is inheritly dynamic, meaning you can spawn new pods or shut them down at any point in time.

Kubernetes has a couple of concepts to help you with that, from `_Service_` objects (which allow you to expose parts of your cluster to the outer world) to `_Ingress_` objects (allowing you to do HTTP load balancing). Again, this will amount to tons and tons of YAML and a fair amount of reading, but at the end of the day Kubernetes allows you to route any traffic your application gets to your cluster and the other way around.

(Fun Ingress Fact: You'll need to install an Ingress controller (there is no standard one being built into Kubernetes), which will do the load-balancing for you .Options are plentyful: On platforms like AWS, you'd simply use ELB, if you go bare-metal Kubernetes you could use https://projectcontour.io/[Contour], etc.)

=== Last but not least: ConfigMaps & Secrets Management

Apart from the myriad things you've already seen Kubernetes do, you can also use it to store configuration key-value pairs, as well as secrets (think database or API credentials).

(By default, secrets are being stored unencrypted, hence the need to follow the _Safely use Secrets_ section https://kubernetes.io/docs/concepts/configuration/secret/[on this page], or even altogether plugging in an external Secrets store, from AWS, GCP's and Azure's solutions, to https://github.com/hashicorp/vault-csi-provider[HashiCorp's Vault].)

=== Enough! Don't these YAML files become a mess?

Well...

If you think of deploying e.g. https://wordpress.org/download/source/[Wordpress] with Kubernetes, then you'll need a `_Deployment_`, as well as a `_ConfigMap_` and probably also `_Secrets_`. And then a couple of other `_Services_` and other objects we haven't touched here yet. Meaning, you'll end up with thousands of lines of YAML. This doesn't make it intrinsically _messy_, but already at that small stage, there is a ton of _DevOps_ complexity involved.

However, you're also a developer and [line-through]#hopefully# not necessarily the one maintaining these files.

Just in case you have to, it helps tremendously to use your IDEs Kubernete's support, https://www.jetbrains.com/help/idea/kubernetes.html[IntelliJ IDEA] in my case, to get coding assistance support for Helm charts, Kustomize files et al. Oh, we haven't talked about them yet. Let's do that. Here's a video, which will get you up to speed with https://www.youtube.com/watch?v=CryOrxL0JA8[IntelliJ's Kubernetes Plugin].

=== What are Helm Charts?

TBD

=== What is Kustomize?

TBD

=== What is Terraform?

TBD

=== How do I do local development with Kubernetes?

For local development, you essentially have two choices.

You could run a local Kubernetes cluster and deploy your application(s) into it. You'd probably use https://minikube.sigs.k8s.io/docs/[Minikube] for that. And because the whole "my application changed-now let's build a container image - and then let's deploy this into my cluster" is rather cumbersome to be done manually, you'll probably also want to use a tool like https://skaffold.dev/[Skaffold] to help you with this. Have a look at https://itnext.io/continuous-development-using-skaffold-for-spring-boot-app-on-a-local-minikube-e455704b587c[this tutorial] to get started with that workflow.
While this setup works, it comes with a fair amount of complexity and/or resource consumption.

This is where the workaround, choice number two, comes in. For local development, you'd essentially ignore Kubernetes and clone whatever config you need into your very own docker-compose.yml file and simply run that.
A much simpler setup, but it comes with the downside of having to maintain two sets of configurations (docker-compose.yml + your K8s manifest files).

If you are already using Kubernetes, please let me know in the comment section down below how you approach local development.

=== Do I really need all of this?

It's a good question and it would be the perfect time to sprinkle in some real-life K8s anecdotes: Sysadmins resource constraining pods to death, so that booting up pods takes endlessly - so long that they are being marked unhealthy and killed, leading to an endless pod-creation-killing loop, but we'll save the long explanation for another time.

As a developer you usually don't have to choice to decide, but here's the big picture:

As mentioned earlier in the article, there is an endless amount of learning material when it comes to just 'hosting' a Kubernetes cluster and we're not just talking about 'self-hosting' it entirely, but also using any of the managed Kubernetes variants. If you have the in-house expertise to:

* handle all this additional complexity
* you can explain all the concepts described in this article in more and better detail to all of your developers
* AND FIRST AND FOREMOST you have legitimate requirements to manage hundreds and thousands of containers dynamically (and no, magic out-of-the-blue-scaling requirements don't count)

... go for Kubernetes. But it is my belief that a huge amount of companies could save themselves a fair amount of time, money & stress with a simpler approach.

=== Common Kubectl Commands

If there is any interested in  `_kubectl_` commands that developers might need, post a comment down below, and I'll add the most frequently used here, as neatly grouped/sorted list.

=== Fin

TBD

small workloads, vs . google sized workloads....while default seems to be k8, do you really NEED this stuff??

== Plan For The Next Revision

Vote in the comment section if you want any of the below or all of them to happen:

* Supply copy-paste commands * K8s files so that readers can follow along
* Potentially: Kubectl Commands
* Potentially: Example on Kubernetes vs Docker Compose side-by-side configs
* Potentially: GitOps

== Acknowledgments & References

If you see something in this article that you'd like to see improved, please file a PR.

Thanks to Maarten Balliauw, Andreas Eisele, Andrei Efimov, Anton Aleksandrov for comments/corrections/discussion. Special thanks to the authors of https://www.oreilly.com/library/view/getting-started-with/9780138057626/[Getting Started with Kubernetes], as well as https://learning.oreilly.com/library/view/learning-helm/9781492083641/[Learning Helm].


== TODO marco
TODO marco:

mention that everything is a manual process editing the files

proper sub-headers, categories

adding helm charts, kustomize and terraform to the mix

docker swarm section..

Also to introduce Helm, there’s one more thing you can talk about: k8s metadata is static, so no easy way to roll a new version of a container. Need to edit the file, save, apply. WHAT IS THIS SHIT! - Helm can help you with this.

What I see a lot about similar articles is that authors associate Kubernetes only with Docker. While majority of Kubernetes users run Docker images indeed, it should be highlighted that Kubernetes runs containers.
I would also state that Docker compose works runs on a single machine by default and in order the make it scalable one needs to run Docker Swarm. Some links about Docker swarm v1 and swarm v2 would be useful.
