= Kubernetes for Lazy Developers
Marco Behler
2024-11-11
:page-layout: layout-guides
:page-image: "TODO"
:page-description: TODO
:page-published: false
:page-tags: ["kuberntes"]
:page-commento_id: /guides/kubernetes-for-lazy-developer

(*Editor’s note*: At ~n words, you probably don't want to try reading this on a mobile device. Bookmark it and come back later.)

== Introduction

If you're a developer who has either never used Kubernetes before or wants to brush up on Kubernetes knowledge so that your DevOps colleagues will be [line-through]#scared# positively surprised, this guide is for you.

*Quick Question*: Why is Kubernetes abbreviated to K8s?

*Answer*: It will magically reveal itself at the end of this guide, but only after you having fully read it. So, let's not waste any more time.

=== How did we get here?

Many developers I've met throughout my career, didn't necessarily care about the "now that I've written the code, it also needs to be run somewhere"-part of your application's life.

Let's start slowly. What does running an application actually mean?

It depends a bit on your programming language ecosystem, but, for example, in terms of Java and modern web applications, you literally compile all your source in one, single, executable .jar file, that you can run with a simple command.

[source,java]
----
java -jar myNewAIStartup.jar
----

For production deployments, you typically also want some sort of properties, to set e.g. database credentials or other secrets. The easiest way in above's Java Spring Boot case would be to have an `_application.properties_` file, but you can also go crazy with external secret management services.

In any case, the command above is literally all you need to run to deploy your application - doesn't matter if you're deploying on bare metal, a VM, a Docker container, with or without Kubernetes or maybe even your Java-powered toaster.

=== Running just one command can't be it. Where is the catch?

Much has been written online about the issues that can arise when deploying your application:

* What if there are library/OS/something version incompatibilities between my DEV environment and my PRD environment?
* What if certain required OS packages are missing?
* What if we don't want deploy our application to a cloud service anymore, but a submarine?

Your mileage with these problems will vary _a lot_ depending on what programming language ecosystem you're using - a topic that is often ignored in online discussions.

With pure Java web-applications, the issues mentioned above (excluding the submarine), are, for a large part, non-issues. You https://www.marcobehler.com/guides/a-guide-to-java-versions-and-features[install a JDK] on a server/vm/container/submarine, run `_java -jar_` and live happily ever after.

With PHP, Python, ahem, Node the picture looks different and you could/can easily run into version incompatibilities across your dev, staging or prod environments. Hence, people yearned for a solution to rid them of these pains: Containers.

=== Hail to the Docker!

Chances are high that you already know what Docker is and how to work with it. (If not and you want to see a Docker specific guide, please post a comment down below! More demand -> Faster publication). The short summary is:

* You build your source code into a deployable, e.g. the `_jar_` file.
* You additionally build a new Docker image, with your `_jar_` file baked in.
* Said Docker image also contains _all_ additional software and configuration options needed to be able to run successfully.

-> Instead of deploying your .jar file, you'll now deploy your Docker image and run a Docker container.

The beauty of this is: As long as you have Docker installed on the target machine (and you have Kernel compatibility between your host OS and the Docker container ) you can run any Docker image you want.

[source,console]
----
docker run --name my-new-ai-startup -p 80:8080 -d mynewaistartup

// yay, your application is deployed

// This will start a Docker container on basis of the `_mynewaistartup_` Docker image.
// That image will, among other things, contain your e.g. -jar file,
// a web application which runs on port 8080, as well as instructions on how to run it.
// Hint: These instructions are `_java -jar mynewaistartup.jar_`.
----

=== What does 'deploying' Docker images mean?

You or your CI/CD server managed to bake your application into a Docker image. But how does that Docker image eventually end up on your target deployment server?

You could theoretically save your Docker image as a .tar file, copy it onto your final server and load id there. But more realistically, you'd push your Docker image to a Docker registry, be that https://hub.docker.com/_/registry[dockerhub], https://aws.amazon.com/ecr/[Amazon ECR] or any of the other gazillion self-hosted container image registries, like https://docs.gitlab.com/ee/user/packages/container_registry/[Gitlab's], for example.

Once your image made it to said registry, you can https://docs.docker.com/engine/reference/commandline/login/[login] to that registry on your targetserver.

[source,console]
----
docker login mysupersecret.registry.com
----

And once you are logged in to your registry, your `_docker run_` will be able to find your custom images.

[source,console]
----
docker run --name my-new-ai-startup -p 80:8080 -d mynewaistartup

//....
SUCCESS!
----

=== Docker Compose: Running more than one container simultaneously

What if your application consists of more than just a single Docker container, say, because you need to run https://www.marcobehler.com/guides/java-microservices-a-practical-guide[98354 microservices]?

https://docs.docker.com/compose/[Docker Compose] to the rescue. You'll define all your services and the dependencies between them (run this one or the other one first) in a good, old, YAML file, a `_compose.yaml_`. Here's an example of such a file, defining two services, a web service and a redis service.

[source,yaml]
----
services:
  web:
    build: .
    ports:
      - "8000:5000"
    volumes:
      - .:/code
      - logvolume01:/var/log
    depends_on:
      - redis
  redis:
    image: redis
volumes:
  logvolume01: {}
----

Then you just run `_docker compose up_` and your whole _environment_ (consisting of all your separate services running in separate docker containers) will be started.

While `_Docker Compose_` might be mainly known for quickly spinning up development or testing environments, it actually works really well for single host deployments as well.
If your application...

* doesn't have any specific high-availability requirements
* you don't mind some manual work (ssh login, docker compose up/down) or using a complementary tool like https://www.ansible.com/[Ansible]
* or you simply don't want to spend enormous amounts of [line-through]#money# investments on a DevOps team

...using Docker Compose for production deployments will go a long way.

=== What do I need Kubernetes for, then?

Things get interesting if you want to start running hundreds, thousands (or a multiple of that) containers, if you don't care or don't want to know on what specific underlying hardware/box your containers will be running on, yet still want to be able to sensibly manage all of this. Kubernetes, to the rescue!

(Note: Quite a while ago I read a Kubernetes book, where in the intro they specified a lower-bound number where Kubernetes makes sense and *I think* it started with hundreds to thousands, though I cannot find the exact book anymore.)

Let's do a quick Kubernetes Concept 101.

TODO GRAPH UML control pane workload..

=== Kubernetes 101: (Worker) Nodes

Your software (or _workload_ in Kubernetes terms) has to run somewhere, be it a virtual or physical machine. Kubernetes call this somewhere `_Nodes_`.

Furthermore, Kubernetes deploys and runs containers: Hello, Docker, my old friend!

Actually, this is not 100% right. In Kubernetes' terms, you deploy (_schedule_) `_Pods_`, with a pod consisting of one or more containers.

Alright, we got `_pods_` running on `_nodes_`, but who controls those nodes and how and where do you decide what to run on these `_nodes_`?

=== Kubernetes 101: Control Pane

Meet the `_Control Plane_` For simplicity's sake, let's just think of it as _one_ component that controls your nodes (as opposed to the https://kubernetes.io/docs/concepts/overview/components/[roughly 947 components] it consists of). The control pane...

* Let's [line-through]#run# _schedule_ your application, i.e. let's put a pod on a node.
* Check if all your pods are in the desired state, e.g. are they responsive or does one of them need to be restarted?
* Fulfills every engineer's wet dream: We need to finally scale 10xfold, let's quickly spin up n-more pods!

=== Kubernetes 101: Clusters & Clouds

Take multiple nodes and your control pane, and you have a cluster.

Take multiple clusters and you can separate your dev, test & production environments or maybe teams, projects or different application types - that's up to you.

Take it even further, and [line-through]#try going# go multi-cloud Kubernetes, running multiple clusters across different private and/or public cloud platforms (Congratulations! What you have achieved is not for the faint of heart)

=== Kubernetes 101: Addons

There are also a fair amount of https://kubernetes.io/docs/concepts/overview/components/#web-ui-dashboard[Kubernetes add-ons].

Most importantly for the developer, there is a https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/[Web UI/Dashboard] which you can use to essentially manage your cluster, deploy your containerized applications etc.

If you're not self-hosting your Kubernetes setup, you'd simply use whatever UI your cloud vendors, like https://cloud.google.com/gcp/[Google Cloud], https://aws.amazon.com/eks/[AWS] or the many others provide.

=== Please, let's stop with the Kubernetes 101

Those four 101-sections above will (hopefully) give you enough of a mental model to get started with Kubernetes and we'll leave it at that with the concepts.

Truth to be told, you'll be shocked if you enter "Kubernetes" on https://learning.oreilly.com . You'll get thousands of results, with many many many of the books being 500+ pages long. Fine reading for a rainy winter day! The good part: you, as a developer, don't have to care about most of that's written in these books, teaching you how to setup, maintain and manage your clusters, though being aware of the sheer complexity of all of this helps.

=== What do I need to do to see all of this in action?

* A Kubernetes installation (we'll talk about that a bit later in more detail)
* YAML, YAML, YAML Files
* A tool to interact with your Kubernetes cluster: `_Kubectl_`

=== Where do I get kubectl

Check out https://kubernetes.io/docs/tasks/tools/[this link], which lists various ways of installing kubectl for your specific operating system.

=== What do I need for kubectl to work?

You'll need a config file, a so-called `_kubeconfig file_`, which lets you access your Kubernetes cluster.

By default, that file is located at `_~/.kube/config_`. It is also important to note that this config file is also being read in by your favorite IDE, like https://www.jetbrains.com/idea/[IntelliJ IDEA], to properly set up its Kubernetes features.

=== Where do I get the kubeconfig file from?

If you are using a managed Kubernetes installation (https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html[EKS], https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl[GKE], https://gist.github.com/dcasati/c71243c1a010993d9f281e0f06dc839d[AKS]), check out the corresponding documentation pages. Yes, just click the links, I did all the work linking to the correct pages. Simply put, you'll use their CLI tools to generate the file for you.

If you installed e.g. https://minikube.sigs.k8s.io/docs/start/[Minikube] locally, it will create the kubeconfig file for you.

If you can ssh into your master node, run a

`_cat /etc/kubernetes/admin.conf_` or cat `_~/.kube/config_`

=== Anything else I need to know about the kubeconfig file?

A kubeconfig file is good, old YAML, and there are many things it can contain (clusters, users, contexts). For the inclined, https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/[check out the official documentation]. For now we can ignore users and contexts and live with the simplification that the kubeconfig file contains the cluster(s) you can connect to, e.g. `_development_` or `_test_`.

Here's an example kubeconfig file, taken https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/[from the official Kubernetes documentation].

[source,yaml]
----
apiVersion: v1
clusters:
- cluster:
    certificate-authority: fake-ca-file
    server: https://1.2.3.4
  name: development
- cluster:
    insecure-skip-tls-verify: true
    server: https://5.6.7.8
  name: test
contexts:
- context:
    cluster: development
    namespace: frontend
    user: developer
  name: dev-frontend
- context:
    cluster: development
    namespace: storage
    user: developer
  name: dev-storage
- context:
    cluster: test
    namespace: default
    user: experimenter
  name: exp-test
current-context: ""
kind: Config
preferences: {}
users:
- name: developer
  user:
    client-certificate: fake-cert-file
    client-key: fake-key-file
- name: experimenter
  user:
    # Documentation note (this comment is NOT part of the command output).
    # Storing passwords in Kubernetes client config is risky.
    # A better alternative would be to use a credential plugin
    # and store the credentials separately.
    # See https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins
    password: some-password
    username: exp
----

=== Kubectl 101

What can you now do with Kubectl? Remember, at the beginning we said your goal is to have a pod (n+ containers), and schedule it (run them) on a node (server).

And the way is to feed yaml files (yay) with the desired state of your cluster into kubectl, and it will happily set your cluster into the desired state.


=== Pod Manifests

You could, for example, create a file called `_marcocodes-pod.yaml_` that looks like this:

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: marcocodes-web
spec:
  containers:
    - image: gcr.io/marco/marcocodes:1.4
      name: marcocodes-web
      ports:
        - containerPort: 8080
          name: http
          protocol: TCP
----

The desired state is to have a pod, based on a dockerimage `_marcocodes:1.4_` and being able to access it on port 8080 via http.

If we now run the following command....

[source,console]
----
 kubectl apply -f marcocodes-pod.yaml
----
...then our yaml file will be submitted to the https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/[Kubernetes API Server] and eventually our Kubernetes system will schedule that Pod to run on a healthy, viable node in our cluster.

We managed to schedule a single, static, one-off pod. Where is the advantage to just running `_docker run -d --publish 8080:8080 gcr.io/marco/marcocodes:1.4_`?
›
Well, for now, there is none.

That's why we unfortunately need to learn two more Kubernetes concepts: `_ReplicaSets_` and `_Deployments_

=== ReplicaSets

What if you want to have redundant instances of your application? What if you want to be able to scale up or shard your application?

Then, instead of endlessly creating Pod manifests and manually spinning up one-off pods, you'd use Kubernetes' `_ReplicaSets_`.

Let's have a look at a `_marcocodes-replica.yaml_` file, that defines such a minimal ReplicaSet.

[source,yaml]
----
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: marcocodes-web
spec:
  replicas: 2
  selector:
    matchLabels:
      app: marcocodes-web
      version: "3.85"
  template:
    metadata:
      labels:
        app: marcocodes-web
        version: "3.85"
    spec:
      containers:
        - name: marcocodes-web
          image: "gcr.io/marco/marcocodes:3.85"
----

Let's step through this one by one.

[source,yaml,indent=0,role=tooth]
----
kind: ReplicaSet
----

In this .yaml file we want to create a `_ReplicaSet_`, not a `_Pod_ or another so-called Kubernetes `_object_`.

[source,yaml,indent=0,role=tooth]
----
metadata:
  name: marcocodes-web
----

We're giving our `_ReplicaSet_` the name `_marcocodes-web_` (and that name is unique in your cluster, for the samed type of resource).

[source,yaml,indent=0,role=tooth]
----
spec:
  replicas: 2
----

Here's the meat: We want to have 2 replicas == pods running at any given time. If we put in 10 here, Kubernetes would make sure to have 10 running at the same time.

[source,yaml,indent=0,role=tooth]
----
 selector:
    matchLabels:
      app: marcocodes-web
      version: "3.85"
----
What is the `_selector_` bit for? We'll actually skip that for a minute, look at the `_template_` section first and then come back to it again.

[source,yaml,indent=0,role=tooth]
----
template:
    metadata:
      labels:
        app: marcocodes-web
        version: "3.85"
    spec:
      containers:
        - name: marcocodes-web
          image: "gcr.io/marco/marcocodes:3.85"
----

We know we want to have 2 replicas running, but *what* replicas, exactly? Hence, we'll need to define a template for the pods to boot up.

[source,yaml,indent=0,role=tooth]
----
spec:
  containers:
    - name: marcocodes-web
      image: "gcr.io/marco/marcocodes:3.85"
----

We're telling Kubernetes that the pod will consist of only one container, called marcocodes-web, with a specific Docker image from the gcr.io repository.

[source,yaml,indent=0,role=tooth]
----
metadata:
  labels:
    app: marcocodes-web
    version: "3.85"
----

We're simply adding some `_app_` and `_version_` metadata to our created pods. And now, the selector bit above might start making sense.

[source,yaml,indent=0,role=tooth]
----
 selector:
    matchLabels:
      app: marcocodes-web
      version: "3.85"
----

When the ReplicaSet is created (or the so-called _reconcilliation loop_ trying to achieve its desired state), it fetches a Pod listing from the Kubernetes API and _filters_ the results by labels. In this case, pods who have the `_app_ set `_marcocodes-web_` and `_version_` to `_3.85_`. And depending on the number of pods being returned, Kubernetes will then spin up or down additional replicas.

That's all there is to it.

[source,console]
----
kubectl apply -f kuard-rs.yaml
----

As you might have guessed, simply feeding this .yaml file to the `_apply_ command of kubectl will do its job.

`_ReplicaSets_` are _almost_ what you'd like to have, but they come with a problem: They are tied to a specific version of your container images and those are actually not really expected to change. And ReplicaSets also don't really help you with the _rolling out process_ (think, zero downtime) of your software.

Hence we need a new concept to help us manage releases of new versions, meet `_Deployments_`.

=== Deployments

we created pods...replicasetse....but all with a static version number....

what about deployments? and being able to rollout new stuff?

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kuard
  labels:
    run: kuard
spec:
  selector:
    matchLabels:
      run: kuard
  replicas: 1
  template:
    metadata:
      labels:
        run: kuard
    spec:
      containers:
      - name: kuard
        image: gcr.io/kuar-demo/kuard-amd64:blue
----

Problem: version number...

You might have guessed correctly....kubectl create -f kuard-deployment.yaml

deployment is controlling a replicaset..
udpating a containre image: change the yaml file...

rollout....


=== Ingress

doesn't matter if using kubernetes or some other crap..you need to make sure this wors..

=== Self-Healing: Lie

being able to restart a pod..yes....it's functionality that has existed in your good old Linux system forever...systemd..whatever

=== Load Balancing

=== Monitoring

Managing multiple versions of your service

=== Common Kubectl Commands

TODO: Neatly grouped/sorted list of commonly used kubectl command for developers.

In general, the right question to ask yourself when designing Pods is “Will these containers work correctly if they land on different machines?” If the answer is no, a Pod is the correct grouping for the containers. If the answer is yes, using multiple Pods is probably the correct solution. In the example at the beginning of this chapter, the two containers interact via a local filesystem. It would be impossible for them to operate correctly if the containers were scheduled on different machines.

Self-healing
Auto scaling
Resource management
self healing: systemd oder supervisord

pods
wie
run containers.....healthchecks...self healing....what does that even mean? ubuntu/upstart since the break of dawn: restart hanging processes

=== What else can Kubernetes do for me? Secrets Management

https://docs.spring.io/spring-cloud-kubernetes/docs/current/reference/html/

=== Don't these YAML files become a mess?

Kustomize / Helm Charts + IDE support

IntelliJ IDEA support

https://www.jetbrains.com/help/idea/kubernetes.html  (IntelliJ IDEA Ultimate)

[link video]

=== what are helm charts...

=== what is kustomize?

=== What is Terraform?

=== DO I really need all of this?

cgroups....resources....limitation......real life -> 0,5 CPUS.....in development needing the latest macbook pro 64 gig

book reference....100s of applications ok...but what about 100thousands.....missing reference (anyone?)

Comparison with ansible.....puppet?

application servers vs kubernetes...

=== Framework Support

spring boot builds directly into docker image.....optimized .....

=== How does Kubernetes influence my local development?

=== How do I do local development with Kubernetes?

twitter poll

For Development...Docker compose.....

docker-compose files...and K8s manifest files...

clulster & skaffold

minikube....

testcontainers...

===  online hype stories && what ifs?????

strive a career in sales and marketing if you can plausibly explain where 5x the traffic will come from tomorrow....

online hype stories vs reality

blog post from jason cohen on

https://longform.asmartbear.com/exponential-growth/

=== moving complexity

reference kubernetes book...2016 devops...study...would like to have a closer look

100000 of books for Kubernetes...100s of pages to set up just networking

Answered: K8s as an abbreviation results from counting the eight letters between the "K" and the "s". https://kubernetes.io/docs/concepts/overview/#:~:text=rapidly%20growing%20ecosystem.-,Kubernetes%20services%2C%20support%2C%20and%20tools%20are%20widely%20available.,the%20Kubernetes%20project%20in%202014[(source)].

Enough with random, boring facts no one will remember, let's start at the very beginning.


=== Fin

small workloads, vs . google sized workloads....while default seems to be k8, do you really NEED this stuff??

== Acknowledgments

Yet to come. Ssend in a PR btw if you don't like something




=== TODO




nenn mir EIN docker image das du OHNE ZU WISSEN WAS ES TUT einfach wo deployen kannst
du musst auf dockerhub die environment variables lesen
du mussst wissen welche volumes und shit du bereitstellen musst
es ist also nicht "doppelklick app.exe"

rl problem: too little reosurces, killing health-checks, oom, architects, etc.
